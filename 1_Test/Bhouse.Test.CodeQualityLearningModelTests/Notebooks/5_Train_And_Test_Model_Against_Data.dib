#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"name":"csharp"}]}}

#!markdown

# Train And Test Model Against Data

This notebook will leverage ML.NET's implementation of AutoAI to determine the best model to fit against our code analysis data.

#!markdown

Install Nuget Packages

#!csharp

#r "nuget:Microsoft.ML.AutoML"

#!markdown

### Import References

#!csharp

using System.IO;
using System.Collections.Generic;
using System.Runtime;
using Microsoft.ML;
using Microsoft.ML.Data;

#!markdown

### Import Our Filepaths For This Project

#!csharp

#!import Configuration/FilePaths.dib

#!markdown

### Infer Our Columnar Data
AutoML offers a method to `InferColumns`, which will automatically create the inference for us based on the data in the file.

We assign the `labelColumnName` to "HasSmell" as that is the column we'd like to predict.

#!csharp

// Initialize MLContext
MLContext mlContext = new MLContext();

// Define data path
var dataPath = Path.GetFullPath(PathToClassMetricsFile);

// Infer column information
ColumnInferenceResults columnInference =
    mlContext.Auto().InferColumns(dataPath, labelColumnName: "HasSmell", groupColumns: false);

#!markdown

#### Load Our Inferred Data

#!csharp

// Create text loader
TextLoader loader = mlContext.Data.CreateTextLoader(columnInference.TextLoaderOptions);

// Load data into IDataView
IDataView data = loader.Load(dataPath);

#!markdown

#### Apply necessary transformations to our data
We need to change the data type of the label we're predicting from `boolean` to `Single`, as it is the type that ML.NET's BinaryClassifier algorithm requires.

#!csharp

// Perform your column type conversion first.
var convertedData = 
    mlContext
        .Transforms
        .Conversion
        .ConvertType("HasSmell", "HasSmell", DataKind.Single)
        .Fit(data)
        .Transform(data);

#!markdown

#### Train Against A Fraction Of Our Data
We split our data where we train against 80% and test against 20% of it.

#!csharp

DataOperationsCatalog.TrainTestData trainValidationData = mlContext.Data.TrainTestSplit(data, testFraction: 0.2);

#!markdown

#### Define Our Pipeline
This defines our AutoML pipeline - which automatically provides all available ML techniques and configures those that are applicable to the training data and its column information.

Some explanation of each line of code:

1. `Featurizer(convertedData, columnInformation: columnInference.ColumnInformation)` 
   - This method applies automatic feature engineering on data provided as IDataView.
2. `Append(ctx.Auto().BinaryClassification(labelColumnName: columnInference.ColumnInformation.LabelColumnName))`
   - This is appending a binary classification task to the pipeline, which is a common machine learning task to distinguish between two classes, commonly referred as positive and negative classes.
   - This is perfect for the prediction we want to make: does code have a smell or not - yes or no?
3. `labelColumnName: columnInference.ColumnInformation.LabelColumnName`
   - This is specifying the column that is the label in the data, i.e., the column that we want to predict

#!csharp

SweepablePipeline pipeline =
    mlContext
        .Auto()
        .Featurizer(data, columnInformation: columnInference.ColumnInformation)
        .Append(mlContext.Auto().BinaryClassification(labelColumnName: columnInference.ColumnInformation.LabelColumnName));

#!markdown

## Configuring Our ML Experiment

This is where we use AutoML on our model to determine the best algorithm available to train with.

This could be considered "Hyper-Parameterization", where we are trying out many different inputs to our ML algorithms to try to find what works best.

#!markdown

#### Create the experiment.

#!csharp

AutoMLExperiment experiment = mlContext.Auto().CreateExperiment();

#!markdown

#### Configure the experiment
We configure our experiment to do the following:
- `SetPipeline(pipeline)`
    - Tells our experiment to use the ML pipeline we configured in previous steps.
- `SetBinaryClassificationMetric(BinaryClassificationMetric.Accuracy, columnInference.ColumnInformation.LabelColumnName)`
    - Tells our experiment to target a BinaryClassification where we want to optimize for the best Accuracy we can find for the `HasSmell` column.
- `SetTrainingTimeInSeconds(20)`
    - Tells our experiment to run for 20 seconds.
    - The longer we run, the more experiments we'll run- providing higher likelihood of finding parameters that provide the best prediction accuracy.
- `SetDataset(trainValidationData)`
    - Tells our experiment to use the training data we partitioned.

#!csharp

experiment
    .SetPipeline(pipeline)
    .SetBinaryClassificationMetric(BinaryClassificationMetric.Accuracy, columnInference.ColumnInformation.LabelColumnName)
    .SetTrainingTimeInSeconds(20)
    .SetDataset(trainValidationData);

#!markdown

#### Add a destination for the trainer's logs to output to.
This is so we can see what the experiment is doing.

We should see it trying out or experimenting with many variations of parameters for finding what combinations lead to higher accuracy.

#!csharp

// Log experiment trials
mlContext.Log += (_, e) => {
    if (e.Source.Equals("AutoMLExperiment"))
    {
        Console.WriteLine(e.RawMessage);
    }
};

#!markdown

#### Run the experiment

#!csharp

TrialResult experimentResults = await experiment.RunAsync();

#!markdown

#### Save Trained Model
This allows us to access the MLModel the experiment built.

This way we don't have to run the experiment again to find and use our model.

#!csharp

//mlContext.Model.Save(experimentResults.Model, loader, PathToTrainedModelFile);

#!markdown

## Test Our Model
Load the model we saved and perform some predictions on our test data to see how accurate the model is.

#!csharp

// Create the ML Context for testing purposes.
MLContext mlContextForTesting = new MLContext();

DataViewSchema predictionPipelineSchema;

// Load the trained model
ITransformer predictionPipeline = mlContextForTesting.Model.Load(PathToTrainedModelFile, out predictionPipelineSchema);

#!csharp

// Apply schema validation on the test data set.
IDataView predictions = predictionPipeline.Transform(trainValidationData.TestSet);

#!markdown

The below codeblock creates an estimator for a binary classification model using the Stochastic Dual Coordinate Ascent algorithm (SDCA). 

Estimators are used in ML.NET to describe how the model should be trained. 

In this case, this is a binary classification model (used when there are exactly two classes to predict), with the "HasSmell" column used as the label (aka what we want to predict).

#!csharp

var sdcaEstimator = mlContext.BinaryClassification.Trainers.SdcaLogisticRegression(labelColumnName: "HasSmell");

#!markdown

The below codeblock is performing a cross-validation of the binary classification model. 

Cross-validation is a procedure used to avoid overfitting (a modeling error that occurs when a function is too closely fit to a limited set of data points) and give an indication of how well your model will perform on unseen data.

It works by splitting your data into a defined number (5 in this case, known as 'folds') of different 'folds' or 'batches'. It then trains your model multiple times on different combinations of these folds and then averages the result. 

Each iteration of training uses a different fold for testing and the remaining folds for training.

#!csharp

var crossValidationResults = mlContextForTesting.BinaryClassification.CrossValidate(predictions, sdcaEstimator, numberOfFolds: 5, labelColumnName: "HasSmell");

#!markdown

#### Display the test results
First just to see what the `sdcaEstimator` looks like.

It contains the configuration for the estimator used for assessing the accuracy of the results of the cross validation.

#!csharp

sdcaEstimator.Display()

#!markdown

Next, we see the results of each fold/batch of our cross validation.

Each batch includes a Metrics collection that tells us various attributes about how the model performed on the test data.

#!csharp

crossValidationResults.Display()
